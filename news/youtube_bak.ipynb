{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 \n",
    "# !pip install --upgrade google-auth-oauthlib\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and APIs\n",
    "import fetch_videos\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "# Ensure you have set your OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Definitions and Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily News Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify channel ID and name\n",
    "channel_id = fetch_videos.channels[\"CNBC_TV\"]  # Change this to the desired channel ID\n",
    "channel_name = fetch_videos.get_channel_name_by_id(channel_id, fetch_videos.channels)\n",
    "summary_file_name_today = f'summaries_{channel_name}_{fetch_videos.get_formated_date_today()}'\n",
    "summary_file_name_today_cvs = f'{summary_file_name_today}.csv'\n",
    "summary_file_name_today_html = f'{summary_file_name_today}.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Fetch videos\n",
    "period_type = 'today'  # 'today', 'days', 'weeks', 'months'\n",
    "number = 1  # The 'today' setting does not use 'number', adjust if using other settings\n",
    "start_date, end_date = fetch_videos.get_date_range(period_type, number)\n",
    "df_videos_today = fetch_videos.fetch_videos(start_date, end_date, channel_id,summary_file_name_today_cvs)\n",
    "\n",
    "# ## Filter Vidoes by time\n",
    "# # Assume df_videos is your initial DataFrame loaded with video data\n",
    "# hour_range1 = '0-8'\n",
    "# hour_range2 = '8-10'\n",
    "# hour_range3 = '10-12'\n",
    "# hour_range4 = '12-14'\n",
    "# hour_range5 = '14-16'\n",
    "# hour_range6 = '16-24'\n",
    "\n",
    "# # Filter videos from the last 3 days\n",
    "# filtered_df = fetch_videos.filter_videos_by_date_and_time(df_videos_today, 'today', 1)\n",
    "# # fetch_videos.display_df(filtered_df)\n",
    "\n",
    "## Add transcripts\n",
    "df_videos_with_transcripts = fetch_videos.add_transcripts_to_df(df_videos_today)\n",
    "# fetch_videos.display_df(df_videos_with_transcripts,include_video_id=True,include_transcript=True)\n",
    "df_videos_with_transcripts.to_csv(summary_file_name_today_cvs, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign Tasks: Get Summary\n",
    "task_summary = \"\"\"I would like you to summarize the transcript with the following instructions: \n",
    "First categorize the video content.The category should be one of the following: Crypto, Macro, Politics, Technology, Small Caps or Other. \n",
    "Then summarize the stocks that are mentioned in this video.\n",
    "Then provide key takeaways in a bullet point format.Please make sure don't miss anything about small cap, Nvidia, Tesla, Meta and Macro is mentioned in the transcript.\n",
    "Please print the summary in a human-readable format like the following: \n",
    "Category: Technology\n",
    "Stock mentioned: STOCK1, STOCK2, STOCK3\n",
    "Key takeaways:\n",
    "* takeaway 1\n",
    "* takeaway 2\n",
    "* takeaway 3\n",
    "\"\"\"\n",
    "# load df_videos_with_transcripts from CSV\n",
    "df_videos_with_transcripts = pd.read_csv(summary_file_name_today_cvs)\n",
    "\n",
    "# Get summaries for all transcripts\n",
    "df_summaries = fetch_videos.apply_tasks_on_all_transcripts(df_videos_with_transcripts, client, task_summary)\n",
    "\n",
    "# Save summaries to a CSV file\n",
    "df_summaries.to_csv(summary_file_name_today_cvs, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save summaries to an HTML file \n",
    "df_summaries = pd.read_csv(summary_file_name_today_cvs)\n",
    "html_content = fetch_videos.get_html_content_summary_only(df_summaries)\n",
    "# print(html_content)\n",
    "\n",
    "# Save the HTML content to an HTML file\n",
    "with open(summary_file_name_today_html, 'w') as file:\n",
    "    file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save titles and transcripts to a text file\n",
    "summary_file_name_today_txt = f'{summary_file_name_today}.txt'\n",
    "fetch_videos.save_videos_to_text(df_summaries, summary_file_name_today_txt,\"Title\", \"Transcript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2 = \"\"\"Please tell me anything discussed in the file about small cap or russell stocks. \n",
    "Please also provide which video or speaker side this\n",
    "\"\"\"\n",
    "with open(summary_file_name_today_txt, 'r') as file:\n",
    "    all_transcripts = file.read()\n",
    "summary = fetch_videos.apply_task(all_transcripts, client, task2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI for all transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel_name='CNBC_TV'\n",
    "summary_file_name_today_cvs='youtube_summary_CNBC_TV_2024-09-23.csv'\n",
    "\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import pandas as pd\n",
    "import os\n",
    "import fetch_videos\n",
    "\n",
    "email_user = os.getenv('EMAIL_USER')\n",
    "email_password = os.getenv('EMAIL_PASSWORD')\n",
    "email_send1 = 'sliu810@gmail.com'\n",
    "email_send2 = 'zhengwang827@gmail.com'\n",
    "\n",
    "recipients = [email_send1,email_send2]\n",
    "# Set up the MIME\n",
    "message = MIMEMultipart()\n",
    "message['From'] = email_user\n",
    "message['To'] = email_send1\n",
    "message['Subject'] = f'summaries_{channel_name}_{fetch_videos.get_formated_date_today()}'\n",
    "\n",
    "# load df_summaries from disk and get the html content\n",
    "df_summaries = pd.read_csv(summary_file_name_today_cvs)\n",
    "html_content= fetch_videos.get_html_content_summary_only(df_summaries)\n",
    "\n",
    "# Attach the HTML content to the email\n",
    "message.attach(MIMEText(html_content, 'html'))\n",
    "\n",
    "# Function to send email\n",
    "def send_email():\n",
    "    try:\n",
    "        server = smtplib.SMTP('smtp-mail.outlook.com', 587)  # Outlook SMTP server\n",
    "        #server.set_debuglevel(1)  # Enable debugging output\n",
    "        server.starttls()\n",
    "        server.login(email_user, email_password)\n",
    "        text = message.as_string()\n",
    "        server.sendmail(email_user, recipients, text)\n",
    "        server.quit()\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n",
    "\n",
    "# Send the email\n",
    "send_email()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Off Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript = fetch_videos.get_transcript('I6_8szN5zrE')\n",
    "# with open('transcript.txt', 'w') as file:\n",
    "#     file.write(all_transcripts)\n",
    "# print(transcript)\n",
    "# # Example task\n",
    "# task2 = \"\"\"I would like you to summarize the text which describes Tesla FSD.\n",
    "# Please say in what senario FSD performed well and waht senario it didn't perform well.\n",
    "# Format like this:\n",
    "# Title: \n",
    "# Key takeaways:\n",
    "# * FSD performed well in ...\n",
    "# * FSD had some challenges in ....\n",
    "# Improvement over pervious version if any.\n",
    "# \"\"\"\n",
    "# task3 = \"\"\"could you print the transcript with two person speaking into a human readible format and maintain the original content?\"\"\"\n",
    "\n",
    "# summary = get_summary(transcript, client, task3)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_summary_in_chunks(transcript, client, task, chunk_size=500):\n",
    "#     \"\"\"\n",
    "#     Generates summaries in chunks to avoid truncation.\n",
    "\n",
    "#     Parameters:\n",
    "#     - transcript (str): The full transcript text.\n",
    "#     - client (object): The client to use for generating summaries.\n",
    "#     - task (str): The task identifier for the summary generation.\n",
    "#     - chunk_size (int): The maximum size of each chunk.\n",
    "\n",
    "#     Returns:\n",
    "#     - str: Combined summary of all chunks.\n",
    "#     \"\"\"\n",
    "#     import textwrap\n",
    "\n",
    "#     # Split the transcript into chunks\n",
    "#     chunks = textwrap.wrap(transcript, chunk_size)\n",
    "#     full_summary = \"\"\n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         summary = get_summary(chunk, client, task)\n",
    "#         full_summary += summary + \"\\n\"\n",
    "\n",
    "#     return full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript = fetch_videos.get_transcript('I8JzsnZVylY')\n",
    "# print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task2 = \"\"\"I would like you to summarize the text which describes Tesla FSD.\n",
    "# Please say in what senario FSD performed well and waht senario it didn't perform well.\n",
    "# Format like this:\n",
    "# Title: \n",
    "# Key takeaways:\n",
    "# * FSD performed well in ...\n",
    "# * FSD had some challenges in ....\n",
    "# Improvement over pervious version if any.\n",
    "# \"\"\"\n",
    "# task3 = \"\"\"\"\"\"\n",
    "\n",
    "# # summary = fetch_videos.apply_task(transcript, client, task3)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module loaded. Use YouTubeSummary(CONFIG).run(channel_name) to run the script or call individual functions.\n",
      "2024-09-23 17:46:23,922 - INFO - Setting channel to CNBC_TV\n",
      "2024-09-23 17:46:23,943 - INFO - Channel set to CNBC_TV (ID: UCrp_UI8XtuYfpiqluWLD7Lw)\n",
      "2024-09-23 17:46:23,943 - INFO - Summaries will be saved in: /Users/sliu/src/mysrc/investing/razorback_investing/news/CNBC_TV\n",
      "2024-09-23 17:46:23,944 - INFO - Setting channel to CNBC_TV\n",
      "2024-09-23 17:46:23,945 - INFO - Channel set to CNBC_TV (ID: UCrp_UI8XtuYfpiqluWLD7Lw)\n",
      "2024-09-23 17:46:23,945 - INFO - Summaries will be saved in: /Users/sliu/src/mysrc/investing/razorback_investing/news/CNBC_TV\n",
      "2024-09-23 17:46:23,945 - INFO - No existing data found. Will start with an empty DataFrame.\n",
      "2024-09-23 17:46:23,947 - INFO - Fetching videos for channel ID: UCrp_UI8XtuYfpiqluWLD7Lw\n",
      "2024-09-23 17:46:24,160 - WARNING - Encountered 403 Forbidden with reason \"quotaExceeded\"\n",
      "2024-09-23 17:46:24,160 - WARNING - YouTube API quota exceeded. Using existing data if available.\n",
      "2024-09-23 17:46:24,161 - WARNING - No videos data available.\n",
      "2024-09-23 17:46:24,161 - ERROR - No videos data available. Make sure to fetch videos first.\n",
      "2024-09-23 17:46:24,162 - ERROR - No videos data available. Make sure to fetch videos and add transcripts first.\n",
      "2024-09-23 17:46:24,162 - ERROR - No data to save. Make sure to fetch videos, add transcripts, and generate summaries first.\n",
      "2024-09-23 17:46:24,162 - ERROR - No data available to send in email.\n",
      "2024-09-23 17:46:24,163 - INFO - Pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "from YouTubeSummary import YouTubeSummary, CONFIG\n",
    "\n",
    "# Use the default configuration\n",
    "yt_summary = YouTubeSummary(CONFIG)\n",
    "yt_summary.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One off study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-off Study Cell for Specific Videos\n",
    "\n",
    "import fetch_videos\n",
    "from YouTubeSummary import YouTubeSummary, CONFIG\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# Specify the video ID(s) you want to study\n",
    "video_ids = [\"pGV5rTJV_jY\"]  # Replace with actual video IDs\n",
    "# If you want to study just one video, you can use: video_ids = [\"SINGLE_VIDEO_ID\"]\n",
    "\n",
    "# Create a YouTubeSummary instance (for using its configuration)\n",
    "yt_summary = YouTubeSummary(CONFIG)\n",
    "\n",
    "# Function to get transcript and generate summary for a single video\n",
    "def process_video(video_id):\n",
    "    # Fetch video details\n",
    "    video_details = fetch_videos.get_video_details(video_id)\n",
    "    \n",
    "    # Get transcript\n",
    "    transcript = fetch_videos.get_transcript(video_id)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_task = CONFIG['summary']['task']\n",
    "    summary = fetch_videos.apply_task(transcript, yt_summary._client, summary_task)\n",
    "    \n",
    "    return {\n",
    "        \"Video ID\": video_id,\n",
    "        \"Title\": video_details['title'],\n",
    "        \"Published At\": video_details['publishedAt'],\n",
    "        \"Transcript\": transcript,\n",
    "        \"Summary\": summary\n",
    "    }\n",
    "\n",
    "# Process each video\n",
    "results = []\n",
    "for video_id in video_ids:\n",
    "    try:\n",
    "        result = process_video(video_id)\n",
    "        results.append(result)\n",
    "        print(f\"Processed video: {result['Title']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_id}: {str(e)}\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "for _, row in result_df.iterrows():\n",
    "    print(f\"Video Title: {row['Title']}\")\n",
    "    print(f\"Video ID: {row['Video ID']}\")\n",
    "    print(f\"Published At: {row['Published At']}\")\n",
    "    print(f\"Transcript: {row['Transcript'][:500]}...\")  # Display first 500 characters of transcript\n",
    "    print(f\"Summary: {row['Summary']}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "# result_df.to_csv(\"one_off_study_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
